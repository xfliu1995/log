# 1.1 数据预处理

## 1\) 缺失值处理

缺失值会使得系统丢失了大量的有用信息，系统所表现出来的不确定性更加显著，系统中蕴含的确定性成分更难把握，包含空值的不完全变量会使得挖掘过程陷入混乱。因此我们需要对数据缺失值进行处理。

### 1a) 直接使用含有缺失值的特征

我们可以把缺失作为一种单独的特征状态，也就是一个特殊值，不进行处理。有些机器学习模型可以处理数据有缺失值的情况，例如决策树模型、xgboost模型等；有些模型不能处理数据有缺失的情况，例如SVM模型。


### 1b) 直接使用含有缺失值的特征

* 删除有缺失数据的样本
* 删除有过多缺失数据的特征。若变量的缺失率较高（大于80%），覆盖率较低，且重要性较低，可以直接将变量删除。

### 1c) 缺失值补全

**(1) 平均值填充**
可以用空缺特征的平均数、中位数、众数、最大值、最小值、固定值等作为填充。

如果空值是数值属性，就使用该属性在其他所有对象的取值的平均值来填充该缺失的属性值.如果空值是非数值属性，就根据统计学中的众数原理，用该属性在其他所有对象出现频率最高的值来补齐该缺失的属

**(2) 热卡填充（就近补齐）**

对于一个包含空值的对象，热卡填充法在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。不同的问题选用不同的标准来对相似进行判定。

**(3) K最近邻法**

先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。

**(4) 回归插补**

回归基于完整的数据集，建立回归方程（模型）。对于包含空值的对象，将已知属性值代入方程来估计未知属性值，以此估计值来进行填充

**(5) EM插补**

在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计。这种方法也被称为忽略缺失值的极大似然估计。
该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。

**(6) 多重插补**

多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。
具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。
多重插补方法分为三个步骤：
> ①为每个空值产生一套可能的插补值，这些值反映了无响应模型的不确定性；每个值都可以被用来插补数据集中的缺失值，产生若干个完整数据集合。
> ②每个插补数据集合都用针对完整数据集的统计方法进行统计分析。
> ③对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值。


## 2\) 数据归一化和标准化
### 2a) 为什么进行特征缩放

在实际应用中，样本的不同特征的单位不同，会在求距离时造成很大的影响。比如： 在两个样本中肿瘤大小的分别为1cm和5cm，发现时间分别为100天和200天，那么在求距离时，时间差为100、大小差为4，那么其结果会被时间所主导，因为肿瘤大小的差距太小了。但是如果我们把时间用年做单位，0.27年与0.55年的差距又远小于肿瘤大小的差距，结果又会被大小主导了。

因此在量纲不同的情况下，可能不能正确反映样本中每一个特征的重要程度。在机器学习实践中，往往有着将不同规格的数据转换到同一规格，或不同的数据转换到相同分布的需求，这种需求统称为将数据**“无量纲化”**。

譬如**梯度和矩阵为核心**的算法中，譬如逻辑回归，支持向量机，神经网络，**无量纲化可以加快求解速度**；而在**距离类模型**，譬如K近邻，K-Means聚类中，**无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响**。

当然，这里有一个特例，就是是对于决策树和树的集成算法，我们不需要无量纲化，决策树可以把任意数据都处理得很好。

### 2b) 特征放缩

**特征缩放是一种用于规范自变量或数据特征范围的方法**。特征缩放的本质是通过**除以一个固定值，将数据固定在某个范围之中**，取对数也算是一种缩放处理。

归一化和标准化只是特征缩放的一个子概念。在数据处理中，也称为**数据规范化**，通常在数据预处理步骤中执行。


常见的特征缩放方法包括：

**(1) standard/z-score scaling**

Standard/z-score scaling first shift features to their centers\(mean\) and then divide by their standard deviation. This method is suitable for most continous features of approximately Gaussian distribution.

$$\text{zscore}(x_{ij}^{'}) = \frac{x_{ij} - \mu _{ij}}{\sigma _i}$$

**(2) min-max scaling**

Min-max scaling method scales data into range \[0, 1\]. This method is suitable for data concentrated within a range and preserves zero values for sparse data. Min-max scaling is also sensitive to outliers in the data. Try removing outliers or clip data into a range before scaling.

$$\text{minmax}(x_{ij}^{'}) = \frac{x_{ij} - \text{min}_k \mathbf{x}_{ik}} {\text{max}_k x_{ik} - \text{min}_k x_{ik}}$$

**(3) abs-max scaling**

Max-abs scaling method is similar to min-max scaling, but scales data into range \[-1, 1\]. It does not shift/center the data and thus preserves signs \(positive/negative\) of features. Like min-max, max-abs is sensitive to outliers.

$$\text{maxabs}(x_{ij}^{'}) = \frac{x_{ij}}{\text{max}_k \vert x_{ik} \vert}$$

**(4) robust scaling**

Robust scaling method use robust statistics \(median, interquartile range\) instead of mean and standard deviation. Median and IQR are less sensitive to outliers. For features with large numbers of outliers or largely deviates from normal distribution, robust scaling is recommended.

$$\text{robustscale}(x_{ij}^{'}) = \frac{x_{ij} - \text{median}_k x_{ik}} {Q_{0.75}(\mathbf{x}_i) - Q_{0.25}(\mathbf{x}_i)}$$


### 2c) 编程实现

在R语言和python中，我们可以通过调用


| Scaling method | Python function | R function |
| :--- | :--- | :--- |
| standard/z-score scaling |  sklearn.preprocessing.StandardScaler | scale(x,center=T,scale=T) |
| min-max scaling |sklearn.preprocessing.minmax_scale  | |
| abs-max scaling | sklearn.preprocessing.maxabs_scale  | |
| robust scaling |  sklearn.preprocessing.robust_scale  | |


下面我们以min-max scaling为例，说明如何利用python完成归一化。
> 在sklearn当中，我们使用`preprocessing.MinMaxScaler`来实现这个功能。`MinMaxScaler`有一个重要参数，`feature_range`，希望把数据压缩到的范围，默认是[0,1]。
> ```python
> from sklearn.preprocessing import MinMaxScaler
> import pandas as pd
> data = [[-1,2],[-0.5,6],[0,10],[1,18]]
> pd.DataFrame(data)
> ```
> 下面就是调用`MinMaxScaler`进行归一化，默认缩放到[0,1]范围内：
> ```python
> scaler = MinMaxScaler() 
> scaler = scaler.fit(data) 
> result = scaler.transform(data)
> result
> # 输出：
> array([[0.  , 0.  ],
>        [0.25, 0.25],
>        [0.5 , 0.5 ],
>        [1.  , 1.  ]])
> ```
> 当然，我们也可以一步到位，将fit和transform联合在一起
> ```python
> result = scaler.fit_transform(data)
> ```
> 如果说我们不想要归一化了，也可以将缩放完的数据还原回去：
> ```python
> scaler.inverse_transform(result)
> # 输出：
> array([[-1. ,  2. ],
>        [-0.5,  6. ],
>        [ 0. , 10. ],
>        [ 1. , 18. ]])
> ```
> 在`MinMaxScaler`有一个重要参数，`feature_range`可以将数据压缩到指定的范围，这里我们尝试一下[-1,1]
> ```python
> scaler = MinMaxScaler(feature_range=[-1,1])
> result = scaler.fit_transform(data)
> result
> # 输出
> array([[-1. , -1. ],
>        [-0.5, -0.5],
>        [ 0. ,  0. ],
>        [ 1. ,  1. ]])
> ```
> 当data中的特征数量非常多的时候，fit操作灰报错，并表示：数据量太大计算不了。此时使用`partial_fit`作为借口：
> ```python
> scaler = scaler.partial_fit(data)
> ```
